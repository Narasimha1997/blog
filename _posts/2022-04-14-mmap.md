---
title: About memory mapped files and Disk I/O
published: true
---
`mmap` is the most useful yet underrated system call because most of the beginner developers don't understand it's potential - this is obvious because memory mapping of files is not as simple as using `read` and `write` system calls over files that reside on the disk. In this post, I have tried to explain memory mapped I/O, why and where it can be useful, how it is faster than `read` and `write` system calls and some caveats. 

### How data is read from the disk?
Before diving into `mmap` we need to understand what happens when we read data from the disk, though it appears as just `open`, `read` and `close`, there are lot of things happening behind the scenes. To explain this, I will create a small C program that opens a file and reads the data.

```C
#include<stdio.h>
#include<stdlib.h>

int main(int argc, char** argv) {
    FILE *fp = fopen("data.txt", "r");
    if (fp != NULL) {
        char* data = (char*)malloc(512);
        fread(data, 1, 512, fp);
        printf("%s", data);
    }
}
```

The above code opens a file called `data.txt`, if it is successful, creates a 512 bytes buffer on heap and then reads first 512 bytes of the file into that heap area from where our program can access it. Now let's understand what exactly happens when we call `read`. (I took this diagram from [here](https://www.oreilly.com/library/view/hands-on-system-programming/9781788998475/63043e6d-0d12-4949-bca8-c43b2bd74a00.xhtml), so the real credits of this diagram goes to O'reilly) 

<div style="text-align: center">
    <img src="./assets/mmap/disk_io.png" alt="drawing"/>
</div>

From the above diagram, we can see that `read()` system call propagates from the user application to the storage hardware. First, when a `read()` system call is made, the CPU switches to protected mode (the mode in which kernel runs) from user-mode and invokes the system call code. The kernel stores all the file descriptors currently opened in a list, that list is searched to obtain the inode information of the file descriptor, this inode contains all the low level details of a file specific to the device it is stored on. Once the inode information is obtained, the corresponding `read()` function associated with that inode is invoked. Since our file is a regular data file that resides on the disk, the `read()` function called on the inode internally invokes the Block I/O subsystem to read the data from underlying storage device. 

The data from disk can be read in fixed block sizes. For example, older ATA disks support 512 bytes block size. SSDs on the other hand allow data to be read as fixed 4KB pages, all these things are abstract to the upper layers because the storage driver and Block I/O subsystem handles all these things. Block I/O subsystem calls the device driver code to read the contents and waits for the driver to complete the read. After read is complete, data is available to the Block I/O subsystem as blocks (or pages in SSD) of fixed sizes. For example, if we want to read 15KB of data, then the disk driver provides 4 pages (each of size 4KB). These pages are cached in a temporary in-memory buffer called **Page Cache**. Let's understand what exactly the **Page cache** is used for.

#### Page cache
Calling the disk driver always to read data from the disk is not so performance friendly because the disk operation will touch lot of electronics other than CPU and memory, to avoid this problem, kernel stores recently used data blocks in a cache which resides on RAM (memory). Each page/block in this cache is identified by the block number. Block I/O subsystem checks this cache for the given block number/s before calling the disk driver, if the block/s is present, it is served directly from memory to avoid calling disk driver unnecessarily. If required blocks are not present in cache, they are read from the disk via disk driver then stored in the cache. In case of a write, the modifications are made directly on the data in cache (if present) and then marked as dirty, kernel periodically flushes the dirty blocks back to the disk, so they are persisted. Page cache behaves like LRU cache, so when size of the page cache is full, old blocks are removed to make room for latest blocks.

Irrespective of whether data is read from disk or from cache, in the end, Block I/O subsystem provides the required data to the upper layers. In other words, Block I/O subsystem returns the pointer to the entry of required data in page cache. This data is then copied to application's buffer which was passed in during the `read` system call (in our case, the buffer was allocated on the application's heap using `malloc`, we passed this buffer pointer as one of the parameters to `fread` function). This is how a `read` operation is performed at high level, the same thing applies to writes as well. Though we use many asynchronous operations like `select`, `epoll` `aio` etc, the underlying mechanism remains the same, the only difference is that the user application is allowed to execute other tasks and is notified back when the operation is complete by Block I/O subsystem.

#### Sequential and Random access
Sequential access involves reading/writing content from a file sequentially - i.e from start to some offset (Example: read first 0-563 bytes). Random access involves reading the contents at random positions - i.e at some arbitrary offsets (Example: read 67-90 bytes). File is always sequential, imagine it as a tape in tape-recorder and there is a metallic nail to go over it - i.e there is no way to access Nth byte of a file before starting from the beginning - this is same for both application and kernel, to access Nth byte we have to first `seek` to that position and then access it. But disk can either be sequential or random - that depends on the type of physical electronics we are using. For example HDD is sequential - we can do random access by moving the read/write head to a particular `(sector, track, block)` combination within a cylinder, however SSDs support Random Access for both reads and writes which is why SSDs are much faster than SSDs (we don't have any mechanical seek like HDDs). Hardware data access is restricted from the user's scope, because driver and drive firmware work together with hardware control unit to do that.

### Caveats of using read and write system calls:
In the approach we discussed in previous section we can clearly see some disadvantages:
1. **System call is made every time**: Imagine we have 1GB of file and we allocate 32KB buffer on heap to read 32KB at a time then we have to make approximately 32768 read system calls. Even if we allocate 1 MB of buffer, we need to make 1024 `read` system calls. Imagine a database system that has 1000 queries per second, we have to make a system call for all the 1000 read requests (few queries may also involve multiple read calls) - this can be a performance bottleneck at large scale.
2. **Lot of context switches**: If we are using a thread pool for I/O, then everytime a thread requests for I/O, it will be preempted by another thread because the thread that requested for I/O blocks. If we are making thousands of such I/O requests every time, we have lot of context switches.
3. **Unnecessary read/writes and seeks during random access**: As stated in previous section, files are sequential in nature, so reading/writing anything in between requires us to either seek manually to that location or read/wrute everything from the beginning. If we are making lot of random accesses, then we might have to seek lot of times. Since seek is also a system call, we are actually adding another unnecessary system call overhead.
4. **Redundant data copy**: As stated in previous section, data is copied to/from page-cache/user-buffer everytime we make read/write system calls. This can be a bottleneck when we have large amounts of data to be read/written.

### Memory mapped files
THe concept of Virtual Memory is outside the scope of this blog